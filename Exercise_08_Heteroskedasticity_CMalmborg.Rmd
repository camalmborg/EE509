---
title: 'Lab 08: Heteroskedasticity'
author: "EE509-Charlotte Malmborg"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives

In this lab we're going to:

* Explore putting process models on variances (heteroskedasticity)
* Explore Bayesian model selection

# Tasks

### Load & Plot Data

```{r}
load("data/Lab08_het.RData")
plot(x,y)
```

# Fit traditional linear model

Start from the basic linear model from lab 5. Fit the model to the data, perform you standard set of Bayesian metrics [trace plots, densities, GBR, pairs, effective sample size, etc.], and plot the resulting model (CI and PI) and data. When simulating your PI, make sure you've got the residual error in the right units (precision vs SD)

```{r}
library(rjags)

## Make the model:
univariate_regression <- "
model {
  b ~ dmnorm(b0,Vb)     ## multivariate Normal prior on vector of regression params
  S ~ dgamma(s1,s2)    ## prior precision

  for(i in 1:n){
      mu[i] <- b[1] + b[2]*x[i]     ## process model
      y[i]  ~ dnorm(mu[i],S)                ## data model
  }
}
"

## Make data object:
data <- list(x = x, y = y, n = length(y))

## Specify priors:
## specify priors
data$b0 <- as.vector(c(0,0))      ## regression b means
data$Vb <- solve(diag(10000,2))   ## regression b precisions
data$s1 <- 0.1                    ## error prior n/2
data$s2 <- 0.1   

## inits:
inits = list(S=1/var(y))

## JAGS model:
j.model   <- jags.model (file = textConnection(univariate_regression),
                             data = data,
                             inits = inits,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("b","S"),
                                n.iter = 10000)

```
```{r}
plot(jags.out)
```
```{r}
GBR <- gelman.plot(jags.out)
```
```{r}
burnin = 2000                              
jags.burn <- window(jags.out,start=burnin)  
plot(jags.burn) 

effectiveSize(jags.burn)
```
```{r}
out <- as.matrix(jags.burn)
pairs(out)
cor(out)
```

```{r}
jags.summ<-summary(jags.burn)

plot(x,y, col='slateblue1', pch=8)

## credible and prediction intervals
niter=nrow(out)
xpred <- seq(0,10,length=30)
npred <- length(xpred)
ypred <- matrix(NA,nrow=niter,ncol=npred)
ycred <- matrix(NA,nrow=niter,ncol=npred)

for(g in 1:niter){
  Ey <- out[g,2] + out[g,3]*xpred
  ycred[g,] <- Ey
  ypred[g,] <- rnorm(npred,Ey,(1/(sqrt(out[g,1]))))
}
ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975))
pi <- apply(ypred,2,quantile,c(0.025,0.975))

lines(xpred,ci[2,],col=3,lwd=2)  ## median model
lines(xpred,ci[1,],col=3,lty=2) ## model CI
lines(xpred,ci[3,],col=3,lty=2)
lines(xpred,pi[1,],col=4,lty=2) ## model PI
lines(xpred,pi[2,],col=4,lty=2)
```

## Calculate model selection metrics

### DIC

```{r}
DIC.ho <- dic.samples(j.model, n.iter=5000)
DIC.ho
```

### WAIC

First, within your JAGS model, add the likelihood calculation within your for loop
```
 like[i] <- dnorm(y[i],mu[i],S)
```
```{r}
univariate_regression_l <- "
model {
  b ~ dmnorm(b0,Vb)     ## multivariate Normal prior on vector of regression params
  S ~ dgamma(s1,s2)    ## prior precision

  for(i in 1:n){
      mu[i] <- b[1] + b[2]*x[i]     ## process model
      y[i]  ~ dnorm(mu[i],S)                ## data model
      like[i] <- dnorm(y[i],mu[i],S)  ##likelihood calculation
  }
}
"

## JAGS model:
j.model   <- jags.model (file = textConnection(univariate_regression_l),
                             data = data,
                             inits = inits,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("b","S","like"),
                                n.iter = 10000)

#plot(jags.out)

```

```{r}
#GBR <- gelman.plot(jags.out.l)
burnin = 2000                              
jags.burn <- window(jags.out,start=burnin)  
out<-as.matrix(jags.burn)

```


Second, assuming that you've converted your JAGS output to a matrix to make the pairs plots and other diagnostics (e.g. `out <- as.matrix(jags.burn)`) we'll want to grab those likelihood columns to calculate WAIC. We'll do that using the `grepl` pattern matching function and the regular expression character `^` which tells R to find any column names that start with the following characters (in this case `like`). Once we do that we'll follow the same calculation as in the  

```{r}
   like   <- out[,grepl("^like",colnames(out))] 
   fbar   <- colMeans(like)
   Pw     <- sum(apply(log(like),2,var))
   WAIC.ho   <- -2*sum(log(fbar))+2*Pw
   WAIC.ho
```
You'll also notice that out output now has a lot of `like` columns that complicate a lot of our other `coda` diagnostics. We can also use `grepl` to _exclude_ all the columns that have a pattern. For example:
```{r}
pairs(out[,!grepl("^like",colnames(out))])
```

### Predictive loss

The code for predictive loss is very similar to our code for generating confidence and predictive intervals, with the biggest difference being that the calculations are done at the OBSERVED X's not a sequence of X's (though if you sort your X's you can often use that sequence to draw the CI & PI). 
```{r}
ngibbs = 3000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
for(g in 1:ngibbs){
  ycred[g,] <- out[g,2] + out[g,3] * xpred
  ypred[g,] <- rnorm(npred,ycred[g,],sqrt(1/out[g,1]))
}
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred
## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.ho <- c(G,P,Dpl)
PL.ho
```
Note: for these metrics I've added `.ho` onto the end of the name for the homoskedastic model. For the heterskedastic model you'll want to change this to something different (e.g. `.he`) so that you don't overwrite the results from your first models (you'll need both to make the table at the end)

# Fit heteroskedastic model 

To add heteroskedasticity, we'll start with the linear regression model and then modify it as follows:

* Within the JAGS `for` loop, add a process model for the calculation of the precision

```
  s[i] <- a[1] + a[2]*x[i]  ## linear model on standard deviation
  S[i] <- 1/s[i]^2          ## calculate precision from SD
```

* Replace prior on `S` with priors on `a[1]` and `a[2]`. To ensure that our variance is always positive, make sure to choose zero-bound prior distributions on `a`. Don't forget to add any new prior parameters to your `data` list.

* Update data model and WAIC likelihood calculation to use `S[i]` instead of a fixed `S`.

* Update your `coda.samples` to include `a` instead of `S`.

```{r}
univariate_regression <- "
model {
  b ~ dmnorm(b0,Vb)     ## multivariate Normal prior on vector of regression params
  a[1] ~ dgamma(s1,s2)   
  a[2] ~ dgamma(s3,s4)

  for(i in 1:n){
      mu[i] <- b[1] + b[2]*x[i]     ## process model
      y[i]  ~ dnorm(mu[i],S[i])                ## data model
      like[i] <- dnorm(y[i],mu[i],S[i])  ##likelihood calculation
      s[i] <- a[1] + a[2]*x[i]  ## linear model on standard deviation
      S[i] <- 1/s[i]^2          ## calculate precision from SD
  }
}
"

## Make data object:
data <- list(x = x, y = y, n = length(y))

## Specify priors:
## specify priors
data$b0 <- as.vector(c(0,0))     
data$Vb <- solve(diag(10000,2))   
data$s1 <- 1                    
data$s2 <- 0.001 
data$s3 <- 0.5
data$s4 <- 0.001

## inits:
inits = list(a=c(1/var(y),0.0001))

## JAGS model:
j.model   <- jags.model (file = textConnection(univariate_regression),
                             data = data,
                             inits = inits,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("b","a","like"),
                                n.iter = 10000)

#splitting jags output code:
codaSplit <- function(jags.out,pattern){
  split = list()
  mfit = as.matrix(jags.out,chains=TRUE)
  pat.cols = grep(pattern,colnames(mfit),fixed=TRUE)
  chain.col = which(colnames(mfit)=="CHAIN")
  split[[1]] = mat2mcmc.list(mfit[,c(chain.col,pat.cols)])
  split[[2]]   = mat2mcmc.list(mfit[,-pat.cols])
  return(split)
}

mat2mcmc.list <- function(w) {
  temp <- list()
  chain.col <- which(colnames(w) == "CHAIN")
  for (i in unique(w[, "CHAIN"])) {
    temp[[i]] <- coda:::as.mcmc(w[w[, "CHAIN"] == i, -chain.col])
  }
  return(as.mcmc.list(temp))
}
```


* As before, perform your standard MCMC metrics & diagnostics

```{r}
j.split <- codaSplit(jags.out,"like")
plot(j.split[[2]])
```
```{r}
GBR <- gelman.plot(j.split[[2]]) 
#
```
```{r}
#burn in:
burnin = 2500                              
jags.burn <- window(jags.out,start=burnin)  
out<-as.matrix(jags.burn)
```


* Calculate your three model selection metrics (DIC, WAIC, PL)
  ** For predictive loss, CI, and PI, don't forget to update your process model to include the process model on sigma and to make sure you're grabbing the right parameters! And don't forget the precision vs SD difference between R and JAGS.
  
```{r}
#DIC:
DIC.he <- dic.samples(j.model, n.iter=5000)
DIC.he
```
```{r}
#WAIC:
like <- out[,grepl("^like",colnames(out))] 
fbar <- colMeans(like)
Pw <- sum(apply(log(like),2,var))
WAIC.he <- -2*sum(log(fbar))+2*Pw
WAIC.he
```
```{r}
#Predictive Loss:
ngibbs = 3000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
for(g in 1:ngibbs){
  ycred[g,] <- out[g,3] + out[g,4] * xpred
  ypred[g,] <- rnorm(npred,ycred[g,],sqrt(1/out[g,1]))
}
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred
## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.he <- c(G,P,Dpl)
PL.he
```

* Plot your model and data with CI and PI

```{r}
jags.summ<-summary(jags.burn)

plot(x,y, col='slateblue1', pch=8, ylim=c(-8,28))

## credible and prediction intervals
niter=nrow(out)
xpred <- seq(0,10,length=30)
npred <- length(xpred)
ypred <- matrix(NA,nrow=niter,ncol=npred)
ycred <- matrix(NA,nrow=niter,ncol=npred)

for(g in 1:niter){
  Ey <- out[g,3] + out[g,4]*xpred
  ycred[g,] <- Ey
  a1 <- out[g,1]
  a2 <- out[g,2]
  ypred[g,] <- rnorm(npred,Ey,((a1+a2*xpred)^2))
}
ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975))
pi <- apply(ypred,2,quantile,c(0.025,0.975))

lines(xpred,ci[2,],col=3,lwd=2)  ## median model
lines(xpred,ci[1,],col=3,lty=2) ## model CI
lines(xpred,ci[3,],col=3,lty=2)
lines(xpred,pi[1,],col=4,lty=2) ## model PI
lines(xpred,pi[2,],col=4,lty=2)
```

* As a final task, make a table that shows the different model selection metrics for both models. Briefly discuss how the metrics performed, what they told us, and where they are the same or different.

```{r}
msdf<-data.frame(matrix(ncol = 4, nrow = 2))
msdf[1,]<-c(sum(DIC.ho$deviance), sum(DIC.ho$penalty), WAIC.ho, PL.ho[3])
msdf[2,]<-c(sum(DIC.he$deviance), sum(DIC.he$penalty), WAIC.he, PL.he[3])
colnames(msdf)<-c("DIC","DIC-penalty","WAIC","Pred Loss")
rownames(msdf)<-c("Homoskedastic Model","Heteroskedastic Model")

print(msdf)

```

>Comparing the two models, my results suggest that the overall performance is improved by accounting for heteroskedasticity. The second model had a lower DIC value and WAIC value, even with a higher number of effective parameters (penalty) in DIC. The predictive loss was higher for the heteroskedatic model, which is the only test of the three that suggested that the homoskedastic model had better predictive performance. This may have been due to the addition of parameters to the model, or my own uninformed bias introduced when choosing prior values for the added parameters a1 and a2 in the heteroskedastic model. In any case, overall the models both perform reasonably well for this data (their values, particularly for DIC and WAIC, are not that different from one another). In all, I think these metrics suggest that the heteroskedastic model is probably a more reasonable choice for modeling these data.

